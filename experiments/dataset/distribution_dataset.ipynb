{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "\n",
    "def contain_question_mark(data):\n",
    "    return data[\"target\"][-1].rstrip() == \"?\"\n",
    "\n",
    "\n",
    "def normalise(data):\n",
    "    # Remove new line characters\n",
    "    data[\"source\"] = data[\"source\"].replace(\"\\n\", \" \")\n",
    "\n",
    "    # Resolve accented characters\n",
    "    data[\"source\"] = \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", data[\"source\"])\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    data[\"target\"] = \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", data[\"target\"])\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def categorise_dataset(data):\n",
    "    target = data[\"target\"].lower()\n",
    "    if any(word in target for word in [\"what\"]):\n",
    "        data[\"category\"] = \"description\"\n",
    "    elif any(\n",
    "        word in target\n",
    "        for word in [\n",
    "            \"how did\",\n",
    "            \"how does\",\n",
    "            \"how do\",\n",
    "            \"compute\",\n",
    "            \"calculate\",\n",
    "            \"how can\",\n",
    "            \"how should\",\n",
    "            \"how would\",\n",
    "            \"how will\",\n",
    "            \"how to\",\n",
    "        ]\n",
    "    ):\n",
    "        data[\"category\"] = \"method\"\n",
    "    elif any(\n",
    "        word in target\n",
    "        for word in [\n",
    "            \"where\",\n",
    "            \"when\",\n",
    "            \"who\",\n",
    "            \"how\",\n",
    "            \"which\",\n",
    "        ]\n",
    "    ):\n",
    "        data[\"category\"] = \"recall\"\n",
    "    elif any(word in target for word in [\"why\"]):\n",
    "        data[\"category\"] = \"explanation\"\n",
    "    else:\n",
    "        data[\"category\"] = \"NA\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_na_category(data):\n",
    "    return data[\"category\"] != \"NA\"\n",
    "\n",
    "\n",
    "def reduce_category_size(dataset, reduceTo, category):\n",
    "    filtered_dataset = dataset.filter(lambda d: d[\"category\"] == category).select(\n",
    "        range(reduceTo)\n",
    "    )\n",
    "    rest_dataset = dataset.filter(lambda d: d[\"category\"] != category)\n",
    "\n",
    "    return concatenate_datasets([filtered_dataset, rest_dataset])\n",
    "\n",
    "\n",
    "def print_distribution(dataset):\n",
    "    categories = [\"method\", \"description\", \"explanation\", \"recall\", \"NA\"]\n",
    "\n",
    "    distributions = []\n",
    "    for category in categories:\n",
    "        category_ds = dataset.filter(lambda data: data[\"category\"] == category)\n",
    "        distribution_str = f\"{category} distribution = {len(category_ds) / len(dataset) * 100}%, count = {len(category_ds)}\"\n",
    "        distributions.append(distribution_str)\n",
    "\n",
    "    for d in distributions:\n",
    "        print(d)\n",
    "\n",
    "\n",
    "def stratify_dataset(dataset):\n",
    "    categories = [\"method\", \"description\", \"explanation\", \"recall\"]\n",
    "    reduceTo = get_lowest_category_count(dataset, categories)\n",
    "\n",
    "    for category in categories:\n",
    "        dataset = reduce_category_size(dataset, reduceTo, category)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_lowest_category_count(dataset, categories):\n",
    "    distributions = []\n",
    "\n",
    "    for category in categories:\n",
    "        category_ds = dataset.filter(lambda data: data[\"category\"] == category)\n",
    "        distribution = len(category_ds)\n",
    "        distributions.append(distribution)\n",
    "\n",
    "    return min(distributions)\n",
    "\n",
    "\n",
    "def fix_encoding_errors(data):\n",
    "    # This pattern matches one or more digits followed by an accented 'a'\n",
    "    pattern = r\"(\\d+)Â\"\n",
    "\n",
    "    # See analysis in narrativeqa_encoding.ipynb\n",
    "    data[\"source\"] = (\n",
    "        data[\"source\"]\n",
    "        .replace(\"â\\x80\\x94\", \", \")\n",
    "        .replace(\"Â\\xa0â\\x80\\x93\", \" -\")\n",
    "        .replace(\"â\\x80\\x93\", \"-\")\n",
    "        .replace(\"â\\x80\\x99\", \"'\")\n",
    "        .replace(\"â\\x80\\x9d\", \"\")\n",
    "        .replace(\"â\\x80\\x9c\", \"\")\n",
    "        .replace(\"Ă˛\", \"\")\n",
    "        .replace(\"Ă\\x89\", \"e\")\n",
    "        .replace(\"ÂŁ\", \"$\")\n",
    "        .replace(\"â\\x80\\x89\", \"\")\n",
    "        .replace(\"Ĺ\\x8d\", \"o\")\n",
    "        .replace(\"â\\x82Ź\", \"€\")\n",
    "    )\n",
    "    data[\"source\"] = re.sub(pattern, r\"\\1\", data[\"source\"])\n",
    "\n",
    "    data[\"target\"] = (\n",
    "        data[\"target\"]\n",
    "        .replace(\"â\\x80\\x94\", \", \")\n",
    "        .replace(\"Â\\xa0â\\x80\\x93\", \" -\")\n",
    "        .replace(\"â\\x80\\x93\", \"-\")\n",
    "        .replace(\"â\\x80\\x99\", \"'\")\n",
    "        .replace(\"â\\x80\\x9d\", \"\")\n",
    "        .replace(\"â\\x80\\x9c\", \"\")\n",
    "        .replace(\"Ă˛\", \"\")\n",
    "        .replace(\"Ă\\x89\", \"e\")\n",
    "        .replace(\"ÂŁ\", \"$\")\n",
    "        .replace(\"â\\x80\\x89\", \"\")\n",
    "        .replace(\"Ĺ\\x8d\", \"o\")\n",
    "        .replace(\"â\\x82Ź\", \"€\")\n",
    "    )\n",
    "    data[\"target\"] = re.sub(pattern, r\"\\1\", data[\"target\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_dataset_name(data, name):\n",
    "    data[\"dataset\"] = name\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'dataset'],\n",
      "    num_rows: 87599\n",
      "})\n",
      "method distribution = 1.3957029267467433%, count = 1155\n",
      "description distribution = 59.05696401382411%, count = 48872\n",
      "explanation distribution = 1.3691181090944244%, count = 1133\n",
      "recall distribution = 38.17821495033473%, count = 31594\n",
      "NA distribution = 0.0%, count = 0\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = (\n",
    "  load_dataset(\"squad\", split=\"train\", trust_remote_code=True)\n",
    "  .select_columns([\"context\", \"question\"])\n",
    "  .rename_columns({\"context\": \"source\", \"question\": \"target\"})\n",
    "  .map(add_dataset_name, fn_kwargs={\"name\": \"squad\"})\n",
    ")\n",
    "\n",
    "print(squad_dataset)\n",
    "\n",
    "squad_dataset = (\n",
    "  squad_dataset.filter(contain_question_mark)\n",
    "  .map(normalise)\n",
    "  .map(categorise_dataset)\n",
    "  .filter(remove_na_category)\n",
    ")\n",
    "\n",
    "print_distribution(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869a4821dbc84016b8871ec740731ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7741b191836d4a48b98ac3f96ebcf394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9696cf98571d40cda011e1c5b5d8d2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7f6a2748854b6b967fd6a74f916ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e29b832cc74f5ba39a3a6f7898908e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/28385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d060cc09511141fbbded1c403f74e51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d147467667244ecacd4da7782382f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4462924e02ab422a9e283a6f9bae933e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e09ae36ca149eb9ca94dcce0fbd1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fbaf9e174e40c2af5a164be4887f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method distribution = 3.8161318300086733%, count = 1012\n",
      "description distribution = 58.380783589124775%, count = 15482\n",
      "explanation distribution = 2.8432444662317584%, count = 754\n",
      "recall distribution = 34.95984011463479%, count = 9271\n",
      "NA distribution = 0.0%, count = 0\n"
     ]
    }
   ],
   "source": [
    "adversarial_dataset = (\n",
    "  load_dataset(\"adversarial_qa\", \"adversarialQA\",  split=\"train\", trust_remote_code=True)\n",
    "  .select_columns([\"context\", \"question\"])\n",
    "  .rename_columns({\"context\": \"source\", \"question\": \"target\"})\n",
    "  .map(add_dataset_name, fn_kwargs={\"name\": \"adversarial\"})\n",
    ")\n",
    "\n",
    "adversarial_dataset = (\n",
    "  adversarial_dataset.filter(contain_question_mark)\n",
    "  .map(normalise)\n",
    "  .map(categorise_dataset)\n",
    "  .filter(remove_na_category)\n",
    ")\n",
    "\n",
    "print_distribution(adversarial_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9985f897184a7487f0812a239fb936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54cd33b8e5b4ca8a425263ec30975eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0c3615c8a9440ebd0872eca368808f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method distribution = 6.675451565211802%, count = 2077\n",
      "description distribution = 42.05180947483448%, count = 13084\n",
      "explanation distribution = 9.847657003278266%, count = 3064\n",
      "recall distribution = 41.425081956675456%, count = 12889\n",
      "NA distribution = 0.0%, count = 0\n"
     ]
    }
   ],
   "source": [
    "narrative_dataset = (\n",
    "  load_dataset(\"narrativeqa\", split=\"train\",trust_remote_code=True)\n",
    "  .select_columns([\"document\", \"question\"])\n",
    "  .map(\n",
    "    lambda x: {\n",
    "        \"document\": x[\"document\"][\"summary\"][\"text\"],\n",
    "        \"question\": x[\"question\"][\"text\"],\n",
    "    }\n",
    "  )\n",
    "  .rename_columns({\"document\": \"source\", \"question\": \"target\"})\n",
    "  .map(fix_encoding_errors)\n",
    "  .map(add_dataset_name, fn_kwargs={\"name\": \"narrative\"})\n",
    ")\n",
    "\n",
    "narrative_dataset = (\n",
    "  narrative_dataset.filter(contain_question_mark)\n",
    "  .map(normalise)\n",
    "  .map(categorise_dataset)\n",
    "  .filter(remove_na_category)\n",
    ")\n",
    "\n",
    "print_distribution(narrative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method distribution = 0.9322373696872494%, count = 93\n",
      "description distribution = 87.48997594226142%, count = 8728\n",
      "explanation distribution = 0.5112269446672013%, count = 51\n",
      "recall distribution = 11.066559743384122%, count = 1104\n",
      "NA distribution = 0.0%, count = 0\n"
     ]
    }
   ],
   "source": [
    "sciq_dataset = (\n",
    "  load_dataset(\"sciq\", split=\"train\", trust_remote_code=True)\n",
    "  .select_columns([\"support\", \"question\"])\n",
    "  .rename_columns({\"support\": \"source\", \"question\": \"target\"})\n",
    "  .filter(lambda x: x[\"source\"] != \"\")\n",
    "  .map(add_dataset_name, fn_kwargs={\"name\": \"sciq\"})\n",
    ")\n",
    "\n",
    "sciq_dataset = (\n",
    "  sciq_dataset.filter(contain_question_mark)\n",
    "  .map(normalise)\n",
    "  .map(categorise_dataset)\n",
    "  .filter(remove_na_category)\n",
    ")\n",
    "\n",
    "print_distribution(sciq_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
