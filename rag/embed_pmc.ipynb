{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PMC176547', \"==== FrontPLoS BiolPLoS BiolpbioplosbiolPLoS Biology1544-91731545-7885Public Library of Science San Francisco, USA 10.1371/journal.pbio.0000007SynopsisEcologyEvolutionGenetics/Genomics/Gene TherapyZoologyMammalsBorneo Elephants: A High Priority for Conservation Synopsis10 2003 18 8 2003 18 8 2003 1 1 e7Copyright: © 2003 Public Library of Science.2003This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.DNA Analysis Indicates That Asian Elephants Are Native to Borneo and Are Therefore a High Priority for Conservation==== BodyA new study settles a long-standing dispute about the genesis of an endangered species. With scant fossil evidence supporting a prehistoric presence, scientists could not say for sure where Borneo's elephants came from. Did they descend from ancient prototypes of the Pleistocene era or from modern relatives introduced just 300–500 years ago? That question, as Fernando et al. report in this issue, is no longer subject to debate.Applying DNA analysis and dating techniques to investigate the elephants' evolutionary path, researchers from the United States, India, and Malaysia, led by Don Melnick of the Center for Environmental Research and Conservation at Columbia, demonstrate that Borneo's elephants are not recent arrivals. They are genetically distinct from other Asian elephants and may have parted ways with their closest Asian cousins when Borneo separated from the mainland, effectively isolating the Borneo elephants some 300,000 years ago.In the 1950s, Borneo elephants had been classified as a subspecies of Asian elephants (either Indian or Sumatran) based on anatomical differences, such as smaller skull size and tusk variations. This classification was later changed, partly because of the popular view that these animals had descended from imported domesticated elephants. Until now, there was no solid evidence to refute this belief and no reason to prioritize the conservation of Borneo elephants.Their new status, as revealed by this study, has profound implications for the fate of Borneo's largest mammals. Wild Asian elephant populations are disappearing as expanding human development disrupts their migration routes, depletes their food sources, and destroys their habitat. Recognizing these elephants as native to Borneo makes their conservation a high priority and gives biologists important clues about how to manage them.Borneo elephant\"]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv_rows(csv_file):\n",
    "    \"\"\"\n",
    "    Opens a CSV file and iteratively reads each row.\n",
    "\n",
    "    Args:\n",
    "    - csv_file (str): The path to the CSV file.\n",
    "\n",
    "    Yields:\n",
    "    - list: Each row of the CSV file as a list.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        # skips field column rows\n",
    "        next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            yield row\n",
    "           \n",
    "\n",
    "# Example usage:\n",
    "file_path = './../data/pmc_1mil.csv'  # Replace 'example.csv' with the path to your CSV file\n",
    "\n",
    "count = 0\n",
    "for row in read_csv_rows(file_path):\n",
    "    print(row)\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Collection test_collection does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chromadb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmc_1mil\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/mmqg/.venv/lib/python3.11/site-packages/chromadb/api/client.py:264\u001b[0m, in \u001b[0;36mClient.delete_collection\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_collection\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    262\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    263\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mmqg/.venv/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/mmqg/.venv/lib/python3.11/site-packages/chromadb/api/segment.py:347\u001b[0m, in \u001b[0;36mSegmentAPI.delete_collection\u001b[0;34m(self, name, tenant, database)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_cache[existing[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Collection test_collection does not exist."
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "client.delete_collection(\"test_collection\")\n",
    "\n",
    "collection = client.create_collection(\"pmc_1mil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_limit = 20\n",
    "\n",
    "from transformers import BartConfig, BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"alinet/bart-base-balanced-qg\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "doc = \"The sun set behind the mountains, casting a golden glow over the serene lake. Birds chirped softly as the day faded into twilight.\"\n",
    "\n",
    "def trim_document_to_token_limit(document, token_limit, tokenizer):\n",
    "  sentences = document.split(\".\")\n",
    "  print(sentences)\n",
    "\n",
    "  while len(tokenizer.tokenize(document)) > token_limit:\n",
    "    sentences.pop()\n",
    "\n",
    "\n",
    "len(tokenizer.tokenize(\"The sun set behind the mountains, casting a golden glow over the serene lake. Birds chirped softly as the day faded into twilight.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun set behind the mountains, casting a golden glow over the serene lake. Birds chirped softly as the day faded into twilight.\n",
      "The sun set behind the mountains \n",
      "The sun set behind the mountains, casting a golden glow over the serene lake. Birds\n",
      "18\n",
      "chirped softly as the day faded into twilight.\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def len_tokenize(string):\n",
    "  return len(tokenizer.tokenize(string))\n",
    "\n",
    "\n",
    "doc = \"The sun set behind the mountains, casting a golden glow over the serene lake. Birds chirped softly as the day faded into twilight.\"\n",
    "doc1 = \"The sun set behind the mountains \"\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len_tokenize,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents([doc])\n",
    "\n",
    "for doc in documents:\n",
    "  print(doc.page_content)\n",
    "  print(len(tokenizer.tokenize(doc.page_content)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual script for generate_embedding_pmc_1mil.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off send_request(...) for 0.6s (requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Read timed out. (read timeout=15))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BartTokenizer\n",
    "import chromadb\n",
    "from angle_emb import AnglE\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "# Initialisations \n",
    "# Tokenizers - model_name needs to be passed as argument to script\n",
    "model_name = \"alinet/bart-base-balanced-qg\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Utility functions\n",
    "def len_tokenize(string):\n",
    "  return len(tokenizer.tokenize(string))\n",
    "\n",
    "def remove_until_last_fullstop(input_string):\n",
    "    # Find the index of the last full stop\n",
    "    last_fullstop_index = input_string.rfind('.')\n",
    "    \n",
    "    # If no full stop is found, return the original string\n",
    "    if last_fullstop_index == -1:\n",
    "        return input_string\n",
    "    \n",
    "    # Return the substring from the beginning of the string to the last full stop\n",
    "    return input_string[:last_fullstop_index + 1]\n",
    "\n",
    "# Text splitter\n",
    "token_limit = 512\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=token_limit,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len_tokenize,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# ChromaDB client and collection\n",
    "client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "collection = client.get_or_create_collection(name=\"pmc_1mil\")\n",
    "\n",
    "# Embedding Model\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC176545C40\n",
      "11.48909330368042\n",
      "PMC176546C14\n",
      "3.6310009956359863\n",
      "PMC176547C1\n",
      "0.49527502059936523\n",
      "PMC176548C1\n",
      "0.5955550670623779\n",
      "PMC193604C20\n",
      "6.057276964187622\n",
      "PMC193605C23\n",
      "7.762468099594116\n",
      "PMC193606C1\n",
      "0.6144440174102783\n",
      "PMC193607C1\n",
      "0.5955069065093994\n",
      "PMC212319C20\n",
      "5.7224040031433105\n",
      "PMC212687C30\n",
      "8.643115043640137\n"
     ]
    }
   ],
   "source": [
    "pmc = (\n",
    "  load_dataset(\"pmc/open_access\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    ")\n",
    "pmc_iter = iter(pmc)\n",
    "\n",
    "for idx, row in enumerate(pmc_iter):\n",
    "  if idx == 10:\n",
    "    break\n",
    "\n",
    "  #remove this line from we load_dataset pmc_1mil\n",
    "  article = row['text'].replace('\\n', '')\n",
    "\n",
    "  start = time.time()\n",
    "  documents = text_splitter.create_documents([article])\n",
    "\n",
    "  chunk_id = \"\"\n",
    "  for id, doc in enumerate(documents):\n",
    "    filtered_page_content = remove_until_last_fullstop(doc.page_content)\n",
    "    embedding = angle.encode(filtered_page_content, to_numpy=True)\n",
    "\n",
    "    chunk_id = row['accession_id'] + \"C\" + str(id)\n",
    "    collection.add(\n",
    "      embeddings=embedding,\n",
    "      documents=filtered_page_content,\n",
    "      ids=chunk_id, \n",
    "    )\n",
    "  end = time.time()\n",
    "  print(chunk_id)\n",
    "  print(end-start)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
