{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import qg\n",
    "import spacy\n",
    "from chunking import ChunkPipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from qg import QGPipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get transcript chunks and  get slide chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the whisper chunks\n",
    "with open(\"experiments/qg/comp3074_lecture_2.pkl\", \"rb\") as file:\n",
    "    whisper_chunks = pickle.load(file)['chunks']\n",
    "\n",
    "# get the slide chunks with timestamps\n",
    "with open('slide_chunks.pkl', 'rb') as file:\n",
    "    slide_chunks = pickle.load(file)\n",
    "\n",
    "# generate trasncript chunks\n",
    "qg_model = qg.Model.DISCORD\n",
    "chunk_pipe = ChunkPipeline(qg_model)\n",
    "transcript_chunks = chunk_pipe(whisper_chunks,2301)\n",
    "\n",
    "# get last endtime in the slide chunkss\n",
    "endtime = slide_chunks[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute the relevant chunk scores, we can pass different similarity functions to this method in order to experiment with the different similarity and embedding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_similarity_scores(similarity_function):\n",
    "    i = 0\n",
    "    relevant_chunks = []\n",
    "\n",
    "    for j, chunk in enumerate(transcript_chunks):\n",
    "        if chunk['timestamp'][0] < endtime:\n",
    "            list_of_slide_indices = []\n",
    "\n",
    "            while i < len(slide_chunks):\n",
    "                list_of_slide_indices.append(i)\n",
    "\n",
    "                if chunk['timestamp'][1] <= slide_chunks[i][2]:\n",
    "                    transcript_chunk_text = chunk['text']\n",
    "                    print(f\"The index of this transcript chunk is: {j}\")\n",
    "\n",
    "                    print(f\"the slide indices that need to be combined are: {list_of_slide_indices}\")\n",
    "                    \n",
    "                    slide_text = \"\"\n",
    "                    for index in list_of_slide_indices:\n",
    "                        slide_text += slide_chunks[index][0]\n",
    "                    print(slide_text)\n",
    "\n",
    "                    cosine_sim = similarity_function(transcript_chunk_text, slide_text)\n",
    "                    relevant_chunks.append(cosine_sim)\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cosine similarity and tfidf vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text1, text2])\n",
    "    cosine_sim = cosine_similarity(vectors)\n",
    "    return cosine_sim[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of this transcript chunk is: 0\n",
      "the slide indices that need to be combined are: [0, 1, 2]\n",
      "COMP3074\n",
      "Human-AI Interaction\n",
      "Lecture 2:\n",
      "Natural Language \n",
      "Processing\n",
      "Dr Jeremie Clos\n",
      "Dr Joel Fischer\n",
      "Lecture 2 – Part 1\n",
      "An Overview of Natural Language Processing\n",
      "▪ Language is at the key of interaction\n",
      "▪ Interaction between people (spoken/sign languages, writing systems)\n",
      "▪ Interaction with machines (command line interface, voice interface, etc.)\n",
      "▪ Interaction between machines (networking protocols)\n",
      "▪ Interaction with animals?\n",
      "▪ Etc.\n",
      "▪ Advances in speech recognition and speech generation (thanks to \n",
      "deep learning1) brought natural language interfaces back!\n",
      "▪ Natural language → languages which naturally evolved over time\n",
      "▪ English, French, Chinese, etc. → natural languages\n",
      "▪ Python, C++ → not natural languages, designed for a purpose\n",
      "Language and interaction\n",
      "1: Learn more at https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\n",
      "\n",
      "The index of this transcript chunk is: 1\n",
      "the slide indices that need to be combined are: [2, 3, 4]\n",
      "▪ Language is at the key of interaction\n",
      "▪ Interaction between people (spoken/sign languages, writing systems)\n",
      "▪ Interaction with machines (command line interface, voice interface, etc.)\n",
      "▪ Interaction between machines (networking protocols)\n",
      "▪ Interaction with animals?\n",
      "▪ Etc.\n",
      "▪ Advances in speech recognition and speech generation (thanks to \n",
      "deep learning1) brought natural language interfaces back!\n",
      "▪ Natural language → languages which naturally evolved over time\n",
      "▪ English, French, Chinese, etc. → natural languages\n",
      "▪ Python, C++ → not natural languages, designed for a purpose\n",
      "Language and interaction\n",
      "1: Learn more at https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\n",
      "▪ Merges insights from multiple fields\n",
      "▪ Linguistics for the application aspect\n",
      "▪ Computer science for the computational aspect\n",
      "▪ Artificial intelligence for the autonomous aspect\n",
      "▪ Two main subfields\n",
      "▪ Natural Language Understanding (NLU)\n",
      "▪ Finding insights and understanding from text\n",
      "▪ “Hey Siri, what time is it?”\n",
      "▪ Natural Language Generation (NLG)\n",
      "▪ Generating text for a purpose\n",
      "▪ “It’s 1:30 PM” \n",
      "Natural language processing\n",
      "▪ Natural language understanding (NLU) systems focus on analysing \n",
      "natural language\n",
      "▪ Humans generate text data at a very fast (and increasing) rate\n",
      "▪ 500 million tweets are sent each day\n",
      "▪ Size of English Wikipedia is 5.6 TB uncompressed\n",
      "▪ Humans are obsessive hoarders of data\n",
      "▪ Insights can be gained from that text data\n",
      "▪ How do people feel about a movie? (sentiment)\n",
      "▪ What are people talking about? (topics)\n",
      "▪ How to translate sentences? (translation)\n",
      "▪ How to undermine democracy? (propaganda)\n",
      "▪ And so much more!\n",
      "Natural language understanding\n",
      "\n",
      "The index of this transcript chunk is: 2\n",
      "the slide indices that need to be combined are: [4, 5]\n",
      "▪ Natural language understanding (NLU) systems focus on analysing \n",
      "natural language\n",
      "▪ Humans generate text data at a very fast (and increasing) rate\n",
      "▪ 500 million tweets are sent each day\n",
      "▪ Size of English Wikipedia is 5.6 TB uncompressed\n",
      "▪ Humans are obsessive hoarders of data\n",
      "▪ Insights can be gained from that text data\n",
      "▪ How do people feel about a movie? (sentiment)\n",
      "▪ What are people talking about? (topics)\n",
      "▪ How to translate sentences? (translation)\n",
      "▪ How to undermine democracy? (propaganda)\n",
      "▪ And so much more!\n",
      "Natural language understanding\n",
      "▪ Natural language generation (NLG) systems focus on generating \n",
      "natural language\n",
      "▪ From natural language (e.g. chatbots)\n",
      "▪ Human query → generated response\n",
      "▪ From structured data (e.g. financial reporting, weather prediction)\n",
      "▪ Input is a structured database\n",
      "▪ Output is a description/narration of the data\n",
      "▪ From media data (e.g. generating image captions)\n",
      "▪ Input is media (e.g. video, image)\n",
      "▪ Output is text (e.g. image caption, lip reading algorithms)\n",
      "Natural language generation\n",
      "\n",
      "The index of this transcript chunk is: 3\n",
      "the slide indices that need to be combined are: [5, 6, 7]\n",
      "▪ Natural language generation (NLG) systems focus on generating \n",
      "natural language\n",
      "▪ From natural language (e.g. chatbots)\n",
      "▪ Human query → generated response\n",
      "▪ From structured data (e.g. financial reporting, weather prediction)\n",
      "▪ Input is a structured database\n",
      "▪ Output is a description/narration of the data\n",
      "▪ From media data (e.g. generating image captions)\n",
      "▪ Input is media (e.g. video, image)\n",
      "▪ Output is text (e.g. image caption, lip reading algorithms)\n",
      "Natural language generation\n",
      "▪ Early work in NLP focused on Rule-Based Systems\n",
      "▪ Rule-based systems can do many things:\n",
      "▪ Extract elements from text (e.g., dates, names) with regular expressions\n",
      "▪ E.g., extract dates in the YYYY-mm-dd format: /(\\d{4}-\\d{2}-\\d{1,2}).*/\n",
      "▪ Classify text in a category based on lexicons\n",
      "▪ E.g., sentiment lexicons have positive/negative scores for a list of words\n",
      "▪ Use conditions, extracted elements, and rules to build dynamically generated \n",
      "reports to send to users\n",
      "▪ E.g., some stock market reports are written by rule-based systems\n",
      "▪ Rule-based systems are still used today, because they are reliable, \n",
      "easy to understand, and require no data\n",
      "NLP: the early years\n",
      "▪ More computing power became available, along with data\n",
      "▪ Gave rise to the classical machine learning era\n",
      "▪ Classical machine learning algorithms are still commonly used today\n",
      "▪ Good trade-off of performance vs time to build\n",
      "▪ Machine learning brought a new obsession with benchmarking\n",
      "▪ Early versions: the Cranfield experiments (in the 1960s) for indexing engines\n",
      "▪ Modern versions: TREC, Kaggle, SemEval, etc\n",
      "▪ Not all good:\n",
      "▪ Researchers maximise performance on a few datasets\n",
      "▪ Stifles original research if suboptimal performance\n",
      "▪ No regard for practicality or ecological/ethical impact\n",
      "Then came data (and computing power)\n",
      "\n",
      "The index of this transcript chunk is: 4\n",
      "the slide indices that need to be combined are: [7, 8]\n",
      "▪ More computing power became available, along with data\n",
      "▪ Gave rise to the classical machine learning era\n",
      "▪ Classical machine learning algorithms are still commonly used today\n",
      "▪ Good trade-off of performance vs time to build\n",
      "▪ Machine learning brought a new obsession with benchmarking\n",
      "▪ Early versions: the Cranfield experiments (in the 1960s) for indexing engines\n",
      "▪ Modern versions: TREC, Kaggle, SemEval, etc\n",
      "▪ Not all good:\n",
      "▪ Researchers maximise performance on a few datasets\n",
      "▪ Stifles original research if suboptimal performance\n",
      "▪ No regard for practicality or ecological/ethical impact\n",
      "Then came data (and computing power)\n",
      "▪ Deep learning craze\n",
      "▪ Convolutional Networks started with images due to their \n",
      "inherently hierarchical structure\n",
      "▪ Recurrent Neural Networks became popular with text due to \n",
      "inherently sequential structure\n",
      "▪ Transformer models came to steal both their lunches\n",
      "▪ Models have hundreds of thousands of parameters\n",
      "▪ NLP research took a weird turn\n",
      "Then came more data (and more computer power)\n",
      "\n",
      "The index of this transcript chunk is: 5\n",
      "the slide indices that need to be combined are: [8, 9, 10]\n",
      "▪ Deep learning craze\n",
      "▪ Convolutional Networks started with images due to their \n",
      "inherently hierarchical structure\n",
      "▪ Recurrent Neural Networks became popular with text due to \n",
      "inherently sequential structure\n",
      "▪ Transformer models came to steal both their lunches\n",
      "▪ Models have hundreds of thousands of parameters\n",
      "▪ NLP research took a weird turn\n",
      "Then came more data (and more computer power)\n",
      "▪ Opinion Mining, Sentiment Analysis\n",
      "▪ E.g. detecting whether a movie review is positive or negative, or detecting \n",
      "positive and negative features of a product\n",
      "▪ Argument Stance Detection\n",
      "▪ For a given pair of sentences A and B, detecting whether A is in agreement \n",
      "or disagreement with B\n",
      "▪ Recognising Textual Entailment\n",
      "▪ For a given pair of sentences A and B, recognising whether A being true \n",
      "implies B being true (“A entails B”)\n",
      "Common subfields of NLP (1)\n",
      "▪ Automated Summarisation\n",
      "▪ Producing a summary of arbitrary length for a piece of text\n",
      "▪ Question Answering\n",
      "▪ Google Assistant, Alexa, Siri, Cortana… Whichever personal assistant your \n",
      "current phone (or car, watch, fridge, etc.) has\n",
      "▪ Information Retrieval\n",
      "▪ Search engines, recommender systems\n",
      "▪ Conversational User Interfaces\n",
      "▪ Chatbots! \n",
      "▪ Also their evil cousins: spambots and propaganda bots\n",
      "Common subfields of NLP (2)\n",
      "\n",
      "The index of this transcript chunk is: 6\n",
      "the slide indices that need to be combined are: [10, 11, 12]\n",
      "▪ Automated Summarisation\n",
      "▪ Producing a summary of arbitrary length for a piece of text\n",
      "▪ Question Answering\n",
      "▪ Google Assistant, Alexa, Siri, Cortana… Whichever personal assistant your \n",
      "current phone (or car, watch, fridge, etc.) has\n",
      "▪ Information Retrieval\n",
      "▪ Search engines, recommender systems\n",
      "▪ Conversational User Interfaces\n",
      "▪ Chatbots! \n",
      "▪ Also their evil cousins: spambots and propaganda bots\n",
      "Common subfields of NLP (2)\n",
      "Lecture 2 – Part 2\n",
      "The NLP pipeline\n",
      "▪ Each unit of interest that we analyse is a document\n",
      "▪ A book can be a document\n",
      "▪ A book chapter can be a document\n",
      "▪ A paragraph can be a document\n",
      "▪ A tweet can be a document\n",
      "▪ A collection of documents of interest is a corpus\n",
      "▪ The Gutenberg project library can be a corpus\n",
      "▪ Any arbitrary subset of a corpus can also be a corpus\n",
      "▪ It’s all about framing what we want to analyse\n",
      "Preliminary notions: text, documents, corpora\n",
      "\n",
      "The index of this transcript chunk is: 7\n",
      "the slide indices that need to be combined are: [12, 13]\n",
      "▪ Each unit of interest that we analyse is a document\n",
      "▪ A book can be a document\n",
      "▪ A book chapter can be a document\n",
      "▪ A paragraph can be a document\n",
      "▪ A tweet can be a document\n",
      "▪ A collection of documents of interest is a corpus\n",
      "▪ The Gutenberg project library can be a corpus\n",
      "▪ Any arbitrary subset of a corpus can also be a corpus\n",
      "▪ It’s all about framing what we want to analyse\n",
      "Preliminary notions: text, documents, corpora\n",
      "Overview of the pre-processing pipeline\n",
      "\n",
      "The index of this transcript chunk is: 8\n",
      "the slide indices that need to be combined are: [13]\n",
      "Overview of the pre-processing pipeline\n",
      "\n",
      "The index of this transcript chunk is: 9\n",
      "the slide indices that need to be combined are: [13, 14, 15, 16]\n",
      "Overview of the pre-processing pipeline\n",
      "▪ A document is made of tokens\n",
      "▪ Tokens can be of arbitrary length\n",
      "▪ Do we care about phrases? \n",
      "“The dog ran-and the cat chased him”\n",
      "▪ Do we care about words?\n",
      "“The-dog-ran-and-the-cat-chased-him”\n",
      "▪ Do we care about syllables?    “The-dog-ran-and-the-cat-cha-sed-him”\n",
      "▪ Do we care about characters? “T-h-e-SPACE-d-o-g-SPACE-r-a-n-(…)”\n",
      "▪ Sometimes we care about characters\n",
      "▪ E.g., spelling correction\n",
      "▪ Sometimes we care about syllables\n",
      "▪ E.g., speech recognition\n",
      "▪ But we usually care about words\n",
      "1. Tokenisation\n",
      "▪ Part of speech (usually abbreviated as POS) tagging gives each \n",
      "word their grammatical function in a sentence\n",
      "▪ Different languages have different parts of speech\n",
      "▪ English has\n",
      "▪ Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Interjection, Numeral, \n",
      "Article, Determiner\n",
      "▪ Different POS parsers have different POS tags for the same \n",
      "language\n",
      "▪ POS tags help disambiguate meaning using grammatical context\n",
      "▪ E.g., “The sailor dogs the hatch.”\n",
      "▪ What is “dogs”?\n",
      "1.\n",
      "Noun. The dog is a domesticated carnivore of the family Canidae.\n",
      "2.\n",
      "Verb. To close the doors and secure them in the closed positions\n",
      "2. Annotation (Part of Speech)\n",
      "▪ Words are inflected during usage\n",
      "▪ Should “dog” and “dogs” be different tokens? \n",
      "▪ Should “is”, “are”, and “be” be different tokens?\n",
      "▪ Two ways to solve this:\n",
      "▪ Lemmatisation: reducing terms to their lemma (dictionary form)\n",
      "▪ Stemming: reducing terms to their word stem (common part of \n",
      "the inflections)\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "In linguistic morphology, inflection is a process of word formation, in which a word is modified \n",
      "to express different grammatical categories such as tense, case, voice, aspect, person, \n",
      "number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, \n",
      "and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, \n",
      "participles, prepositions and postpositions, numerals, articles etc., as declension.\n",
      "From: Wikipedia\n",
      "\n",
      "The index of this transcript chunk is: 10\n",
      "the slide indices that need to be combined are: [16, 17, 18]\n",
      "▪ Words are inflected during usage\n",
      "▪ Should “dog” and “dogs” be different tokens? \n",
      "▪ Should “is”, “are”, and “be” be different tokens?\n",
      "▪ Two ways to solve this:\n",
      "▪ Lemmatisation: reducing terms to their lemma (dictionary form)\n",
      "▪ Stemming: reducing terms to their word stem (common part of \n",
      "the inflections)\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "In linguistic morphology, inflection is a process of word formation, in which a word is modified \n",
      "to express different grammatical categories such as tense, case, voice, aspect, person, \n",
      "number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, \n",
      "and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, \n",
      "participles, prepositions and postpositions, numerals, articles etc., as declension.\n",
      "From: Wikipedia\n",
      "▪ Lemmatisation:\n",
      "▪ Dogs → dog\n",
      "▪ Dog → dog\n",
      "▪ dog → dog\n",
      "▪ Lemmatisation produces more consistent content, but is slower\n",
      "▪ Often requires text to be Part of Speech-annotated\n",
      "▪ Stemming:\n",
      "▪ programming → program\n",
      "▪ programmer → programm\n",
      "▪ Stemming produces a lot of words that don’t exist, but is faster\n",
      "▪ Simply runs a bunch of transformation on character strings\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "▪ Not all words matter equally\n",
      "▪ Some words are noise (e.g. determinants)\n",
      "▪ We want to use a smaller vocabulary\n",
      "▪ Easier to build models and make predictions\n",
      "▪ Computationally less expensive\n",
      "▪ Less important words are kept in stop-lists\n",
      "▪ We can also filter words by frequency\n",
      "▪ What defines a “useless” word from an NLP perspective?\n",
      "▪ One that only appears once or twice (not really representative)\n",
      "▪ One that appears everywhere (no discriminative power)\n",
      "4. Filtering: stop words\n",
      "\n",
      "The index of this transcript chunk is: 11\n",
      "the slide indices that need to be combined are: [18, 19]\n",
      "▪ Not all words matter equally\n",
      "▪ Some words are noise (e.g. determinants)\n",
      "▪ We want to use a smaller vocabulary\n",
      "▪ Easier to build models and make predictions\n",
      "▪ Computationally less expensive\n",
      "▪ Less important words are kept in stop-lists\n",
      "▪ We can also filter words by frequency\n",
      "▪ What defines a “useless” word from an NLP perspective?\n",
      "▪ One that only appears once or twice (not really representative)\n",
      "▪ One that appears everywhere (no discriminative power)\n",
      "4. Filtering: stop words\n",
      "▪ Should “Big Ben” be 1 or 2 tokens? What about “Empire State \n",
      "Building”?\n",
      "▪ Analysing words separately is useful, but we might want to use \n",
      "sequences of words\n",
      "▪ A set of consecutive words is called an n-gram\n",
      "▪ Unigram: single word\n",
      "▪ Bigram: two consecutive words\n",
      "▪ Trigram: three consecutive words\n",
      "▪ 4-gram: 4 consecutive words\n",
      "▪ Etc. \n",
      "▪ Only limited by sparsity of representation (more on that later!)\n",
      "Words are cool, but what about pairs of words?\n",
      "\n",
      "[0.13935636174542096, 0.20810105609626248, 0.30331727372819217, 0.3136159539992854, 0.1692906811689349, 0.2708466306878146, 0.2278208149700252, 0.19971738020946844, 0.12529564472501387, 0.36182889890856557, 0.4032467141647036, 0.267555699347143]\n"
     ]
    }
   ],
   "source": [
    "print(compute_similarity_scores(compute_cosine_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using jaccard Similarity and count vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_similarity(text1, text2):\n",
    "    # Create a CountVectorizer to convert text to a bag-of-words representation\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text1, text2]).toarray()\n",
    "\n",
    "    # Compute Jaccard similarity between the two text vectors\n",
    "    intersection = sum(min(v1, v2) for v1, v2 in zip(vectors[0], vectors[1]))\n",
    "    union = sum(max(v1, v2) for v1, v2 in zip(vectors[0], vectors[1]))\n",
    "\n",
    "    jaccard_sim = intersection / union if union != 0 else 0\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of this transcript chunk is: 0\n",
      "the slide indices that need to be combined are: [0, 1, 2]\n",
      "COMP3074\n",
      "Human-AI Interaction\n",
      "Lecture 2:\n",
      "Natural Language \n",
      "Processing\n",
      "Dr Jeremie Clos\n",
      "Dr Joel Fischer\n",
      "Lecture 2 – Part 1\n",
      "An Overview of Natural Language Processing\n",
      "▪ Language is at the key of interaction\n",
      "▪ Interaction between people (spoken/sign languages, writing systems)\n",
      "▪ Interaction with machines (command line interface, voice interface, etc.)\n",
      "▪ Interaction between machines (networking protocols)\n",
      "▪ Interaction with animals?\n",
      "▪ Etc.\n",
      "▪ Advances in speech recognition and speech generation (thanks to \n",
      "deep learning1) brought natural language interfaces back!\n",
      "▪ Natural language → languages which naturally evolved over time\n",
      "▪ English, French, Chinese, etc. → natural languages\n",
      "▪ Python, C++ → not natural languages, designed for a purpose\n",
      "Language and interaction\n",
      "1: Learn more at https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\n",
      "\n",
      "The index of this transcript chunk is: 1\n",
      "the slide indices that need to be combined are: [2, 3, 4]\n",
      "▪ Language is at the key of interaction\n",
      "▪ Interaction between people (spoken/sign languages, writing systems)\n",
      "▪ Interaction with machines (command line interface, voice interface, etc.)\n",
      "▪ Interaction between machines (networking protocols)\n",
      "▪ Interaction with animals?\n",
      "▪ Etc.\n",
      "▪ Advances in speech recognition and speech generation (thanks to \n",
      "deep learning1) brought natural language interfaces back!\n",
      "▪ Natural language → languages which naturally evolved over time\n",
      "▪ English, French, Chinese, etc. → natural languages\n",
      "▪ Python, C++ → not natural languages, designed for a purpose\n",
      "Language and interaction\n",
      "1: Learn more at https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\n",
      "▪ Merges insights from multiple fields\n",
      "▪ Linguistics for the application aspect\n",
      "▪ Computer science for the computational aspect\n",
      "▪ Artificial intelligence for the autonomous aspect\n",
      "▪ Two main subfields\n",
      "▪ Natural Language Understanding (NLU)\n",
      "▪ Finding insights and understanding from text\n",
      "▪ “Hey Siri, what time is it?”\n",
      "▪ Natural Language Generation (NLG)\n",
      "▪ Generating text for a purpose\n",
      "▪ “It’s 1:30 PM” \n",
      "Natural language processing\n",
      "▪ Natural language understanding (NLU) systems focus on analysing \n",
      "natural language\n",
      "▪ Humans generate text data at a very fast (and increasing) rate\n",
      "▪ 500 million tweets are sent each day\n",
      "▪ Size of English Wikipedia is 5.6 TB uncompressed\n",
      "▪ Humans are obsessive hoarders of data\n",
      "▪ Insights can be gained from that text data\n",
      "▪ How do people feel about a movie? (sentiment)\n",
      "▪ What are people talking about? (topics)\n",
      "▪ How to translate sentences? (translation)\n",
      "▪ How to undermine democracy? (propaganda)\n",
      "▪ And so much more!\n",
      "Natural language understanding\n",
      "\n",
      "The index of this transcript chunk is: 2\n",
      "the slide indices that need to be combined are: [4, 5]\n",
      "▪ Natural language understanding (NLU) systems focus on analysing \n",
      "natural language\n",
      "▪ Humans generate text data at a very fast (and increasing) rate\n",
      "▪ 500 million tweets are sent each day\n",
      "▪ Size of English Wikipedia is 5.6 TB uncompressed\n",
      "▪ Humans are obsessive hoarders of data\n",
      "▪ Insights can be gained from that text data\n",
      "▪ How do people feel about a movie? (sentiment)\n",
      "▪ What are people talking about? (topics)\n",
      "▪ How to translate sentences? (translation)\n",
      "▪ How to undermine democracy? (propaganda)\n",
      "▪ And so much more!\n",
      "Natural language understanding\n",
      "▪ Natural language generation (NLG) systems focus on generating \n",
      "natural language\n",
      "▪ From natural language (e.g. chatbots)\n",
      "▪ Human query → generated response\n",
      "▪ From structured data (e.g. financial reporting, weather prediction)\n",
      "▪ Input is a structured database\n",
      "▪ Output is a description/narration of the data\n",
      "▪ From media data (e.g. generating image captions)\n",
      "▪ Input is media (e.g. video, image)\n",
      "▪ Output is text (e.g. image caption, lip reading algorithms)\n",
      "Natural language generation\n",
      "\n",
      "The index of this transcript chunk is: 3\n",
      "the slide indices that need to be combined are: [5, 6, 7]\n",
      "▪ Natural language generation (NLG) systems focus on generating \n",
      "natural language\n",
      "▪ From natural language (e.g. chatbots)\n",
      "▪ Human query → generated response\n",
      "▪ From structured data (e.g. financial reporting, weather prediction)\n",
      "▪ Input is a structured database\n",
      "▪ Output is a description/narration of the data\n",
      "▪ From media data (e.g. generating image captions)\n",
      "▪ Input is media (e.g. video, image)\n",
      "▪ Output is text (e.g. image caption, lip reading algorithms)\n",
      "Natural language generation\n",
      "▪ Early work in NLP focused on Rule-Based Systems\n",
      "▪ Rule-based systems can do many things:\n",
      "▪ Extract elements from text (e.g., dates, names) with regular expressions\n",
      "▪ E.g., extract dates in the YYYY-mm-dd format: /(\\d{4}-\\d{2}-\\d{1,2}).*/\n",
      "▪ Classify text in a category based on lexicons\n",
      "▪ E.g., sentiment lexicons have positive/negative scores for a list of words\n",
      "▪ Use conditions, extracted elements, and rules to build dynamically generated \n",
      "reports to send to users\n",
      "▪ E.g., some stock market reports are written by rule-based systems\n",
      "▪ Rule-based systems are still used today, because they are reliable, \n",
      "easy to understand, and require no data\n",
      "NLP: the early years\n",
      "▪ More computing power became available, along with data\n",
      "▪ Gave rise to the classical machine learning era\n",
      "▪ Classical machine learning algorithms are still commonly used today\n",
      "▪ Good trade-off of performance vs time to build\n",
      "▪ Machine learning brought a new obsession with benchmarking\n",
      "▪ Early versions: the Cranfield experiments (in the 1960s) for indexing engines\n",
      "▪ Modern versions: TREC, Kaggle, SemEval, etc\n",
      "▪ Not all good:\n",
      "▪ Researchers maximise performance on a few datasets\n",
      "▪ Stifles original research if suboptimal performance\n",
      "▪ No regard for practicality or ecological/ethical impact\n",
      "Then came data (and computing power)\n",
      "\n",
      "The index of this transcript chunk is: 4\n",
      "the slide indices that need to be combined are: [7, 8]\n",
      "▪ More computing power became available, along with data\n",
      "▪ Gave rise to the classical machine learning era\n",
      "▪ Classical machine learning algorithms are still commonly used today\n",
      "▪ Good trade-off of performance vs time to build\n",
      "▪ Machine learning brought a new obsession with benchmarking\n",
      "▪ Early versions: the Cranfield experiments (in the 1960s) for indexing engines\n",
      "▪ Modern versions: TREC, Kaggle, SemEval, etc\n",
      "▪ Not all good:\n",
      "▪ Researchers maximise performance on a few datasets\n",
      "▪ Stifles original research if suboptimal performance\n",
      "▪ No regard for practicality or ecological/ethical impact\n",
      "Then came data (and computing power)\n",
      "▪ Deep learning craze\n",
      "▪ Convolutional Networks started with images due to their \n",
      "inherently hierarchical structure\n",
      "▪ Recurrent Neural Networks became popular with text due to \n",
      "inherently sequential structure\n",
      "▪ Transformer models came to steal both their lunches\n",
      "▪ Models have hundreds of thousands of parameters\n",
      "▪ NLP research took a weird turn\n",
      "Then came more data (and more computer power)\n",
      "\n",
      "The index of this transcript chunk is: 5\n",
      "the slide indices that need to be combined are: [8, 9, 10]\n",
      "▪ Deep learning craze\n",
      "▪ Convolutional Networks started with images due to their \n",
      "inherently hierarchical structure\n",
      "▪ Recurrent Neural Networks became popular with text due to \n",
      "inherently sequential structure\n",
      "▪ Transformer models came to steal both their lunches\n",
      "▪ Models have hundreds of thousands of parameters\n",
      "▪ NLP research took a weird turn\n",
      "Then came more data (and more computer power)\n",
      "▪ Opinion Mining, Sentiment Analysis\n",
      "▪ E.g. detecting whether a movie review is positive or negative, or detecting \n",
      "positive and negative features of a product\n",
      "▪ Argument Stance Detection\n",
      "▪ For a given pair of sentences A and B, detecting whether A is in agreement \n",
      "or disagreement with B\n",
      "▪ Recognising Textual Entailment\n",
      "▪ For a given pair of sentences A and B, recognising whether A being true \n",
      "implies B being true (“A entails B”)\n",
      "Common subfields of NLP (1)\n",
      "▪ Automated Summarisation\n",
      "▪ Producing a summary of arbitrary length for a piece of text\n",
      "▪ Question Answering\n",
      "▪ Google Assistant, Alexa, Siri, Cortana… Whichever personal assistant your \n",
      "current phone (or car, watch, fridge, etc.) has\n",
      "▪ Information Retrieval\n",
      "▪ Search engines, recommender systems\n",
      "▪ Conversational User Interfaces\n",
      "▪ Chatbots! \n",
      "▪ Also their evil cousins: spambots and propaganda bots\n",
      "Common subfields of NLP (2)\n",
      "\n",
      "The index of this transcript chunk is: 6\n",
      "the slide indices that need to be combined are: [10, 11, 12]\n",
      "▪ Automated Summarisation\n",
      "▪ Producing a summary of arbitrary length for a piece of text\n",
      "▪ Question Answering\n",
      "▪ Google Assistant, Alexa, Siri, Cortana… Whichever personal assistant your \n",
      "current phone (or car, watch, fridge, etc.) has\n",
      "▪ Information Retrieval\n",
      "▪ Search engines, recommender systems\n",
      "▪ Conversational User Interfaces\n",
      "▪ Chatbots! \n",
      "▪ Also their evil cousins: spambots and propaganda bots\n",
      "Common subfields of NLP (2)\n",
      "Lecture 2 – Part 2\n",
      "The NLP pipeline\n",
      "▪ Each unit of interest that we analyse is a document\n",
      "▪ A book can be a document\n",
      "▪ A book chapter can be a document\n",
      "▪ A paragraph can be a document\n",
      "▪ A tweet can be a document\n",
      "▪ A collection of documents of interest is a corpus\n",
      "▪ The Gutenberg project library can be a corpus\n",
      "▪ Any arbitrary subset of a corpus can also be a corpus\n",
      "▪ It’s all about framing what we want to analyse\n",
      "Preliminary notions: text, documents, corpora\n",
      "\n",
      "The index of this transcript chunk is: 7\n",
      "the slide indices that need to be combined are: [12, 13]\n",
      "▪ Each unit of interest that we analyse is a document\n",
      "▪ A book can be a document\n",
      "▪ A book chapter can be a document\n",
      "▪ A paragraph can be a document\n",
      "▪ A tweet can be a document\n",
      "▪ A collection of documents of interest is a corpus\n",
      "▪ The Gutenberg project library can be a corpus\n",
      "▪ Any arbitrary subset of a corpus can also be a corpus\n",
      "▪ It’s all about framing what we want to analyse\n",
      "Preliminary notions: text, documents, corpora\n",
      "Overview of the pre-processing pipeline\n",
      "\n",
      "The index of this transcript chunk is: 8\n",
      "the slide indices that need to be combined are: [13]\n",
      "Overview of the pre-processing pipeline\n",
      "\n",
      "The index of this transcript chunk is: 9\n",
      "the slide indices that need to be combined are: [13, 14, 15, 16]\n",
      "Overview of the pre-processing pipeline\n",
      "▪ A document is made of tokens\n",
      "▪ Tokens can be of arbitrary length\n",
      "▪ Do we care about phrases? \n",
      "“The dog ran-and the cat chased him”\n",
      "▪ Do we care about words?\n",
      "“The-dog-ran-and-the-cat-chased-him”\n",
      "▪ Do we care about syllables?    “The-dog-ran-and-the-cat-cha-sed-him”\n",
      "▪ Do we care about characters? “T-h-e-SPACE-d-o-g-SPACE-r-a-n-(…)”\n",
      "▪ Sometimes we care about characters\n",
      "▪ E.g., spelling correction\n",
      "▪ Sometimes we care about syllables\n",
      "▪ E.g., speech recognition\n",
      "▪ But we usually care about words\n",
      "1. Tokenisation\n",
      "▪ Part of speech (usually abbreviated as POS) tagging gives each \n",
      "word their grammatical function in a sentence\n",
      "▪ Different languages have different parts of speech\n",
      "▪ English has\n",
      "▪ Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Interjection, Numeral, \n",
      "Article, Determiner\n",
      "▪ Different POS parsers have different POS tags for the same \n",
      "language\n",
      "▪ POS tags help disambiguate meaning using grammatical context\n",
      "▪ E.g., “The sailor dogs the hatch.”\n",
      "▪ What is “dogs”?\n",
      "1.\n",
      "Noun. The dog is a domesticated carnivore of the family Canidae.\n",
      "2.\n",
      "Verb. To close the doors and secure them in the closed positions\n",
      "2. Annotation (Part of Speech)\n",
      "▪ Words are inflected during usage\n",
      "▪ Should “dog” and “dogs” be different tokens? \n",
      "▪ Should “is”, “are”, and “be” be different tokens?\n",
      "▪ Two ways to solve this:\n",
      "▪ Lemmatisation: reducing terms to their lemma (dictionary form)\n",
      "▪ Stemming: reducing terms to their word stem (common part of \n",
      "the inflections)\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "In linguistic morphology, inflection is a process of word formation, in which a word is modified \n",
      "to express different grammatical categories such as tense, case, voice, aspect, person, \n",
      "number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, \n",
      "and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, \n",
      "participles, prepositions and postpositions, numerals, articles etc., as declension.\n",
      "From: Wikipedia\n",
      "\n",
      "The index of this transcript chunk is: 10\n",
      "the slide indices that need to be combined are: [16, 17, 18]\n",
      "▪ Words are inflected during usage\n",
      "▪ Should “dog” and “dogs” be different tokens? \n",
      "▪ Should “is”, “are”, and “be” be different tokens?\n",
      "▪ Two ways to solve this:\n",
      "▪ Lemmatisation: reducing terms to their lemma (dictionary form)\n",
      "▪ Stemming: reducing terms to their word stem (common part of \n",
      "the inflections)\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "In linguistic morphology, inflection is a process of word formation, in which a word is modified \n",
      "to express different grammatical categories such as tense, case, voice, aspect, person, \n",
      "number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, \n",
      "and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, \n",
      "participles, prepositions and postpositions, numerals, articles etc., as declension.\n",
      "From: Wikipedia\n",
      "▪ Lemmatisation:\n",
      "▪ Dogs → dog\n",
      "▪ Dog → dog\n",
      "▪ dog → dog\n",
      "▪ Lemmatisation produces more consistent content, but is slower\n",
      "▪ Often requires text to be Part of Speech-annotated\n",
      "▪ Stemming:\n",
      "▪ programming → program\n",
      "▪ programmer → programm\n",
      "▪ Stemming produces a lot of words that don’t exist, but is faster\n",
      "▪ Simply runs a bunch of transformation on character strings\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "▪ Not all words matter equally\n",
      "▪ Some words are noise (e.g. determinants)\n",
      "▪ We want to use a smaller vocabulary\n",
      "▪ Easier to build models and make predictions\n",
      "▪ Computationally less expensive\n",
      "▪ Less important words are kept in stop-lists\n",
      "▪ We can also filter words by frequency\n",
      "▪ What defines a “useless” word from an NLP perspective?\n",
      "▪ One that only appears once or twice (not really representative)\n",
      "▪ One that appears everywhere (no discriminative power)\n",
      "4. Filtering: stop words\n",
      "\n",
      "The index of this transcript chunk is: 11\n",
      "the slide indices that need to be combined are: [18, 19]\n",
      "▪ Not all words matter equally\n",
      "▪ Some words are noise (e.g. determinants)\n",
      "▪ We want to use a smaller vocabulary\n",
      "▪ Easier to build models and make predictions\n",
      "▪ Computationally less expensive\n",
      "▪ Less important words are kept in stop-lists\n",
      "▪ We can also filter words by frequency\n",
      "▪ What defines a “useless” word from an NLP perspective?\n",
      "▪ One that only appears once or twice (not really representative)\n",
      "▪ One that appears everywhere (no discriminative power)\n",
      "4. Filtering: stop words\n",
      "▪ Should “Big Ben” be 1 or 2 tokens? What about “Empire State \n",
      "Building”?\n",
      "▪ Analysing words separately is useful, but we might want to use \n",
      "sequences of words\n",
      "▪ A set of consecutive words is called an n-gram\n",
      "▪ Unigram: single word\n",
      "▪ Bigram: two consecutive words\n",
      "▪ Trigram: three consecutive words\n",
      "▪ 4-gram: 4 consecutive words\n",
      "▪ Etc. \n",
      "▪ Only limited by sparsity of representation (more on that later!)\n",
      "Words are cool, but what about pairs of words?\n",
      "\n",
      "[0.05615047799930349, 0.10647049747725855, 0.09931176363294138, 0.11702073932873738, 0.07220244239962925, 0.07875526488897058, 0.08874000039070995, 0.06803963266628628, 0.03956687954642075, 0.12353446839549455, 0.13981652977829875, 0.11071600996552473]\n"
     ]
    }
   ],
   "source": [
    "print(compute_similarity_scores(compute_jaccard_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word embeddings instead of tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.10/site-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity_word_embeddings(text1, text2):\n",
    "    # Load spaCy model with word embeddings\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Get the word embeddings for each text\n",
    "    embeddings1 = nlp(text1).vector.reshape(1, -1)\n",
    "    embeddings2 = nlp(text2).vector.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the two vectors\n",
    "    cosine_sim = cosine_similarity(embeddings1, embeddings2)[0, 0]\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of this transcript chunk is: 0\n",
      "the slide indices that need to be combined are: [0, 1, 2]\n",
      "COMP3074\n",
      "Human-AI Interaction\n",
      "Lecture 2:\n",
      "Natural Language \n",
      "Processing\n",
      "Dr Jeremie Clos\n",
      "Dr Joel Fischer\n",
      "Lecture 2 – Part 1\n",
      "An Overview of Natural Language Processing\n",
      "▪ Language is at the key of interaction\n",
      "▪ Interaction between people (spoken/sign languages, writing systems)\n",
      "▪ Interaction with machines (command line interface, voice interface, etc.)\n",
      "▪ Interaction between machines (networking protocols)\n",
      "▪ Interaction with animals?\n",
      "▪ Etc.\n",
      "▪ Advances in speech recognition and speech generation (thanks to \n",
      "deep learning1) brought natural language interfaces back!\n",
      "▪ Natural language → languages which naturally evolved over time\n",
      "▪ English, French, Chinese, etc. → natural languages\n",
      "▪ Python, C++ → not natural languages, designed for a purpose\n",
      "Language and interaction\n",
      "1: Learn more at https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\n",
      "\n",
      "The index of this transcript chunk is: 1\n",
      "the slide indices that need to be combined are: [2, 3, 4]\n",
      "▪ Language is at the key of interaction\n",
      "▪ Interaction between people (spoken/sign languages, writing systems)\n",
      "▪ Interaction with machines (command line interface, voice interface, etc.)\n",
      "▪ Interaction between machines (networking protocols)\n",
      "▪ Interaction with animals?\n",
      "▪ Etc.\n",
      "▪ Advances in speech recognition and speech generation (thanks to \n",
      "deep learning1) brought natural language interfaces back!\n",
      "▪ Natural language → languages which naturally evolved over time\n",
      "▪ English, French, Chinese, etc. → natural languages\n",
      "▪ Python, C++ → not natural languages, designed for a purpose\n",
      "Language and interaction\n",
      "1: Learn more at https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\n",
      "▪ Merges insights from multiple fields\n",
      "▪ Linguistics for the application aspect\n",
      "▪ Computer science for the computational aspect\n",
      "▪ Artificial intelligence for the autonomous aspect\n",
      "▪ Two main subfields\n",
      "▪ Natural Language Understanding (NLU)\n",
      "▪ Finding insights and understanding from text\n",
      "▪ “Hey Siri, what time is it?”\n",
      "▪ Natural Language Generation (NLG)\n",
      "▪ Generating text for a purpose\n",
      "▪ “It’s 1:30 PM” \n",
      "Natural language processing\n",
      "▪ Natural language understanding (NLU) systems focus on analysing \n",
      "natural language\n",
      "▪ Humans generate text data at a very fast (and increasing) rate\n",
      "▪ 500 million tweets are sent each day\n",
      "▪ Size of English Wikipedia is 5.6 TB uncompressed\n",
      "▪ Humans are obsessive hoarders of data\n",
      "▪ Insights can be gained from that text data\n",
      "▪ How do people feel about a movie? (sentiment)\n",
      "▪ What are people talking about? (topics)\n",
      "▪ How to translate sentences? (translation)\n",
      "▪ How to undermine democracy? (propaganda)\n",
      "▪ And so much more!\n",
      "Natural language understanding\n",
      "\n",
      "The index of this transcript chunk is: 2\n",
      "the slide indices that need to be combined are: [4, 5]\n",
      "▪ Natural language understanding (NLU) systems focus on analysing \n",
      "natural language\n",
      "▪ Humans generate text data at a very fast (and increasing) rate\n",
      "▪ 500 million tweets are sent each day\n",
      "▪ Size of English Wikipedia is 5.6 TB uncompressed\n",
      "▪ Humans are obsessive hoarders of data\n",
      "▪ Insights can be gained from that text data\n",
      "▪ How do people feel about a movie? (sentiment)\n",
      "▪ What are people talking about? (topics)\n",
      "▪ How to translate sentences? (translation)\n",
      "▪ How to undermine democracy? (propaganda)\n",
      "▪ And so much more!\n",
      "Natural language understanding\n",
      "▪ Natural language generation (NLG) systems focus on generating \n",
      "natural language\n",
      "▪ From natural language (e.g. chatbots)\n",
      "▪ Human query → generated response\n",
      "▪ From structured data (e.g. financial reporting, weather prediction)\n",
      "▪ Input is a structured database\n",
      "▪ Output is a description/narration of the data\n",
      "▪ From media data (e.g. generating image captions)\n",
      "▪ Input is media (e.g. video, image)\n",
      "▪ Output is text (e.g. image caption, lip reading algorithms)\n",
      "Natural language generation\n",
      "\n",
      "The index of this transcript chunk is: 3\n",
      "the slide indices that need to be combined are: [5, 6, 7]\n",
      "▪ Natural language generation (NLG) systems focus on generating \n",
      "natural language\n",
      "▪ From natural language (e.g. chatbots)\n",
      "▪ Human query → generated response\n",
      "▪ From structured data (e.g. financial reporting, weather prediction)\n",
      "▪ Input is a structured database\n",
      "▪ Output is a description/narration of the data\n",
      "▪ From media data (e.g. generating image captions)\n",
      "▪ Input is media (e.g. video, image)\n",
      "▪ Output is text (e.g. image caption, lip reading algorithms)\n",
      "Natural language generation\n",
      "▪ Early work in NLP focused on Rule-Based Systems\n",
      "▪ Rule-based systems can do many things:\n",
      "▪ Extract elements from text (e.g., dates, names) with regular expressions\n",
      "▪ E.g., extract dates in the YYYY-mm-dd format: /(\\d{4}-\\d{2}-\\d{1,2}).*/\n",
      "▪ Classify text in a category based on lexicons\n",
      "▪ E.g., sentiment lexicons have positive/negative scores for a list of words\n",
      "▪ Use conditions, extracted elements, and rules to build dynamically generated \n",
      "reports to send to users\n",
      "▪ E.g., some stock market reports are written by rule-based systems\n",
      "▪ Rule-based systems are still used today, because they are reliable, \n",
      "easy to understand, and require no data\n",
      "NLP: the early years\n",
      "▪ More computing power became available, along with data\n",
      "▪ Gave rise to the classical machine learning era\n",
      "▪ Classical machine learning algorithms are still commonly used today\n",
      "▪ Good trade-off of performance vs time to build\n",
      "▪ Machine learning brought a new obsession with benchmarking\n",
      "▪ Early versions: the Cranfield experiments (in the 1960s) for indexing engines\n",
      "▪ Modern versions: TREC, Kaggle, SemEval, etc\n",
      "▪ Not all good:\n",
      "▪ Researchers maximise performance on a few datasets\n",
      "▪ Stifles original research if suboptimal performance\n",
      "▪ No regard for practicality or ecological/ethical impact\n",
      "Then came data (and computing power)\n",
      "\n",
      "The index of this transcript chunk is: 4\n",
      "the slide indices that need to be combined are: [7, 8]\n",
      "▪ More computing power became available, along with data\n",
      "▪ Gave rise to the classical machine learning era\n",
      "▪ Classical machine learning algorithms are still commonly used today\n",
      "▪ Good trade-off of performance vs time to build\n",
      "▪ Machine learning brought a new obsession with benchmarking\n",
      "▪ Early versions: the Cranfield experiments (in the 1960s) for indexing engines\n",
      "▪ Modern versions: TREC, Kaggle, SemEval, etc\n",
      "▪ Not all good:\n",
      "▪ Researchers maximise performance on a few datasets\n",
      "▪ Stifles original research if suboptimal performance\n",
      "▪ No regard for practicality or ecological/ethical impact\n",
      "Then came data (and computing power)\n",
      "▪ Deep learning craze\n",
      "▪ Convolutional Networks started with images due to their \n",
      "inherently hierarchical structure\n",
      "▪ Recurrent Neural Networks became popular with text due to \n",
      "inherently sequential structure\n",
      "▪ Transformer models came to steal both their lunches\n",
      "▪ Models have hundreds of thousands of parameters\n",
      "▪ NLP research took a weird turn\n",
      "Then came more data (and more computer power)\n",
      "\n",
      "The index of this transcript chunk is: 5\n",
      "the slide indices that need to be combined are: [8, 9, 10]\n",
      "▪ Deep learning craze\n",
      "▪ Convolutional Networks started with images due to their \n",
      "inherently hierarchical structure\n",
      "▪ Recurrent Neural Networks became popular with text due to \n",
      "inherently sequential structure\n",
      "▪ Transformer models came to steal both their lunches\n",
      "▪ Models have hundreds of thousands of parameters\n",
      "▪ NLP research took a weird turn\n",
      "Then came more data (and more computer power)\n",
      "▪ Opinion Mining, Sentiment Analysis\n",
      "▪ E.g. detecting whether a movie review is positive or negative, or detecting \n",
      "positive and negative features of a product\n",
      "▪ Argument Stance Detection\n",
      "▪ For a given pair of sentences A and B, detecting whether A is in agreement \n",
      "or disagreement with B\n",
      "▪ Recognising Textual Entailment\n",
      "▪ For a given pair of sentences A and B, recognising whether A being true \n",
      "implies B being true (“A entails B”)\n",
      "Common subfields of NLP (1)\n",
      "▪ Automated Summarisation\n",
      "▪ Producing a summary of arbitrary length for a piece of text\n",
      "▪ Question Answering\n",
      "▪ Google Assistant, Alexa, Siri, Cortana… Whichever personal assistant your \n",
      "current phone (or car, watch, fridge, etc.) has\n",
      "▪ Information Retrieval\n",
      "▪ Search engines, recommender systems\n",
      "▪ Conversational User Interfaces\n",
      "▪ Chatbots! \n",
      "▪ Also their evil cousins: spambots and propaganda bots\n",
      "Common subfields of NLP (2)\n",
      "\n",
      "The index of this transcript chunk is: 6\n",
      "the slide indices that need to be combined are: [10, 11, 12]\n",
      "▪ Automated Summarisation\n",
      "▪ Producing a summary of arbitrary length for a piece of text\n",
      "▪ Question Answering\n",
      "▪ Google Assistant, Alexa, Siri, Cortana… Whichever personal assistant your \n",
      "current phone (or car, watch, fridge, etc.) has\n",
      "▪ Information Retrieval\n",
      "▪ Search engines, recommender systems\n",
      "▪ Conversational User Interfaces\n",
      "▪ Chatbots! \n",
      "▪ Also their evil cousins: spambots and propaganda bots\n",
      "Common subfields of NLP (2)\n",
      "Lecture 2 – Part 2\n",
      "The NLP pipeline\n",
      "▪ Each unit of interest that we analyse is a document\n",
      "▪ A book can be a document\n",
      "▪ A book chapter can be a document\n",
      "▪ A paragraph can be a document\n",
      "▪ A tweet can be a document\n",
      "▪ A collection of documents of interest is a corpus\n",
      "▪ The Gutenberg project library can be a corpus\n",
      "▪ Any arbitrary subset of a corpus can also be a corpus\n",
      "▪ It’s all about framing what we want to analyse\n",
      "Preliminary notions: text, documents, corpora\n",
      "\n",
      "The index of this transcript chunk is: 7\n",
      "the slide indices that need to be combined are: [12, 13]\n",
      "▪ Each unit of interest that we analyse is a document\n",
      "▪ A book can be a document\n",
      "▪ A book chapter can be a document\n",
      "▪ A paragraph can be a document\n",
      "▪ A tweet can be a document\n",
      "▪ A collection of documents of interest is a corpus\n",
      "▪ The Gutenberg project library can be a corpus\n",
      "▪ Any arbitrary subset of a corpus can also be a corpus\n",
      "▪ It’s all about framing what we want to analyse\n",
      "Preliminary notions: text, documents, corpora\n",
      "Overview of the pre-processing pipeline\n",
      "\n",
      "The index of this transcript chunk is: 8\n",
      "the slide indices that need to be combined are: [13]\n",
      "Overview of the pre-processing pipeline\n",
      "\n",
      "The index of this transcript chunk is: 9\n",
      "the slide indices that need to be combined are: [13, 14, 15, 16]\n",
      "Overview of the pre-processing pipeline\n",
      "▪ A document is made of tokens\n",
      "▪ Tokens can be of arbitrary length\n",
      "▪ Do we care about phrases? \n",
      "“The dog ran-and the cat chased him”\n",
      "▪ Do we care about words?\n",
      "“The-dog-ran-and-the-cat-chased-him”\n",
      "▪ Do we care about syllables?    “The-dog-ran-and-the-cat-cha-sed-him”\n",
      "▪ Do we care about characters? “T-h-e-SPACE-d-o-g-SPACE-r-a-n-(…)”\n",
      "▪ Sometimes we care about characters\n",
      "▪ E.g., spelling correction\n",
      "▪ Sometimes we care about syllables\n",
      "▪ E.g., speech recognition\n",
      "▪ But we usually care about words\n",
      "1. Tokenisation\n",
      "▪ Part of speech (usually abbreviated as POS) tagging gives each \n",
      "word their grammatical function in a sentence\n",
      "▪ Different languages have different parts of speech\n",
      "▪ English has\n",
      "▪ Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Interjection, Numeral, \n",
      "Article, Determiner\n",
      "▪ Different POS parsers have different POS tags for the same \n",
      "language\n",
      "▪ POS tags help disambiguate meaning using grammatical context\n",
      "▪ E.g., “The sailor dogs the hatch.”\n",
      "▪ What is “dogs”?\n",
      "1.\n",
      "Noun. The dog is a domesticated carnivore of the family Canidae.\n",
      "2.\n",
      "Verb. To close the doors and secure them in the closed positions\n",
      "2. Annotation (Part of Speech)\n",
      "▪ Words are inflected during usage\n",
      "▪ Should “dog” and “dogs” be different tokens? \n",
      "▪ Should “is”, “are”, and “be” be different tokens?\n",
      "▪ Two ways to solve this:\n",
      "▪ Lemmatisation: reducing terms to their lemma (dictionary form)\n",
      "▪ Stemming: reducing terms to their word stem (common part of \n",
      "the inflections)\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "In linguistic morphology, inflection is a process of word formation, in which a word is modified \n",
      "to express different grammatical categories such as tense, case, voice, aspect, person, \n",
      "number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, \n",
      "and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, \n",
      "participles, prepositions and postpositions, numerals, articles etc., as declension.\n",
      "From: Wikipedia\n",
      "\n",
      "The index of this transcript chunk is: 10\n",
      "the slide indices that need to be combined are: [16, 17, 18]\n",
      "▪ Words are inflected during usage\n",
      "▪ Should “dog” and “dogs” be different tokens? \n",
      "▪ Should “is”, “are”, and “be” be different tokens?\n",
      "▪ Two ways to solve this:\n",
      "▪ Lemmatisation: reducing terms to their lemma (dictionary form)\n",
      "▪ Stemming: reducing terms to their word stem (common part of \n",
      "the inflections)\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "In linguistic morphology, inflection is a process of word formation, in which a word is modified \n",
      "to express different grammatical categories such as tense, case, voice, aspect, person, \n",
      "number, gender, mood, animacy, and definiteness. The inflection of verbs is called conjugation, \n",
      "and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, \n",
      "participles, prepositions and postpositions, numerals, articles etc., as declension.\n",
      "From: Wikipedia\n",
      "▪ Lemmatisation:\n",
      "▪ Dogs → dog\n",
      "▪ Dog → dog\n",
      "▪ dog → dog\n",
      "▪ Lemmatisation produces more consistent content, but is slower\n",
      "▪ Often requires text to be Part of Speech-annotated\n",
      "▪ Stemming:\n",
      "▪ programming → program\n",
      "▪ programmer → programm\n",
      "▪ Stemming produces a lot of words that don’t exist, but is faster\n",
      "▪ Simply runs a bunch of transformation on character strings\n",
      "3. Word standardisation: lemmatising vs stemming\n",
      "▪ Not all words matter equally\n",
      "▪ Some words are noise (e.g. determinants)\n",
      "▪ We want to use a smaller vocabulary\n",
      "▪ Easier to build models and make predictions\n",
      "▪ Computationally less expensive\n",
      "▪ Less important words are kept in stop-lists\n",
      "▪ We can also filter words by frequency\n",
      "▪ What defines a “useless” word from an NLP perspective?\n",
      "▪ One that only appears once or twice (not really representative)\n",
      "▪ One that appears everywhere (no discriminative power)\n",
      "4. Filtering: stop words\n",
      "\n",
      "The index of this transcript chunk is: 11\n",
      "the slide indices that need to be combined are: [18, 19]\n",
      "▪ Not all words matter equally\n",
      "▪ Some words are noise (e.g. determinants)\n",
      "▪ We want to use a smaller vocabulary\n",
      "▪ Easier to build models and make predictions\n",
      "▪ Computationally less expensive\n",
      "▪ Less important words are kept in stop-lists\n",
      "▪ We can also filter words by frequency\n",
      "▪ What defines a “useless” word from an NLP perspective?\n",
      "▪ One that only appears once or twice (not really representative)\n",
      "▪ One that appears everywhere (no discriminative power)\n",
      "4. Filtering: stop words\n",
      "▪ Should “Big Ben” be 1 or 2 tokens? What about “Empire State \n",
      "Building”?\n",
      "▪ Analysing words separately is useful, but we might want to use \n",
      "sequences of words\n",
      "▪ A set of consecutive words is called an n-gram\n",
      "▪ Unigram: single word\n",
      "▪ Bigram: two consecutive words\n",
      "▪ Trigram: three consecutive words\n",
      "▪ 4-gram: 4 consecutive words\n",
      "▪ Etc. \n",
      "▪ Only limited by sparsity of representation (more on that later!)\n",
      "Words are cool, but what about pairs of words?\n",
      "\n",
      "[0.4070797, 0.63080883, 0.56591886, 0.6583812, 0.7433294, 0.63322276, 0.7421772, 0.7009978, 0.5173513, 0.752879, 0.7874681, 0.7268001]\n"
     ]
    }
   ],
   "source": [
    "print(compute_similarity_scores(compute_cosine_similarity_word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zawaralilearning/Documents/University/Year 4/DISS/mmqg/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the concept of an NLP pipeline?', 'What was the number of students in the room?', 'What does the field of natural language processing do?', 'What does natural language generation do?', 'What is the evolution of NLP?', 'What was the era of the classical machine learning algorithms?', 'What is the current state of NLP research?', 'What does the system do when it detects a question?', 'What is the use of NLP?', 'What does a document look like?', 'What does tokenization do?', 'What does part of speech mean?', 'What is the goal of standardization?', 'What does the last step do?', 'What was the name of the paper?', 'What was the basis for the word spelling correction in Microsoft Word?', 'What does the process of the process use?', 'What will the lab be doing?']\n"
     ]
    }
   ],
   "source": [
    "qg_pipe = QGPipeline(qg_model)\n",
    "text_chunks = [chunk[\"text\"] for chunk in transcript_chunks]\n",
    "generated_questions = qg_pipe(text_chunks)\n",
    "print(generated_questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07286029c4efa9b37c544460c5fa52c864476e2690f0f794bd6d9982a0a601a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
